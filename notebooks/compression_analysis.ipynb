{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4d1f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from slippi_db import make_compression_datasets\n",
    "from slippi_db import upload_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d7d6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = 'compression_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dce85a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "slp_db = upload_lib.get_db(env, upload_lib.SLP)\n",
    "slp_infos = list(slp_db.find({}))\n",
    "slp_compressed_size = sum(info['stored_size'] for info in slp_infos)\n",
    "slp_size = sum(info['original_size'] for info in slp_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa5516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_size(dataset: str):\n",
    "    parsed_db = upload_lib.get_db(env, dataset)\n",
    "    infos = parsed_db.find({})\n",
    "    sizes = [info['size'] for info in infos if not info.get('failed', False)]\n",
    "    return sum(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd5650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression = slp_infos[0]['compression']\n",
    "for info in slp_infos:\n",
    "    assert info['compression'] == compression\n",
    "print(compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98ff455",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = {name: get_dataset_size(name) for name in make_compression_datasets.configurations}\n",
    "sizes[f'slp_{compression}_compressed'] = slp_compressed_size\n",
    "sizes['slp'] = slp_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64088525",
   "metadata": {},
   "outputs": [],
   "source": [
    "names, sizes_ = zip(*sizes.items())\n",
    "df = pd.DataFrame({'name': names, 'size': sizes_})\n",
    "df['relative_size'] = df['size'] / sizes['uncompressed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db208432",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='relative_size', inplace=True)\n",
    "df[['name', 'relative_size']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bde9bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download test files\n",
    "slp_dir = f'data/{env}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61229068",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(slp_dir, exist_ok=True)\n",
    "for info in slp_infos:\n",
    "    upload_lib.download_slp_locally(env, info['key'], slp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a85bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, zlib, os\n",
    "from slippi_db import parse_libmelee, parse_peppi\n",
    "from slippi_ai.types import InvalidGameError, array_to_nest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9561a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [os.path.join(slp_dir, f) for f in os.listdir(slp_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937a8747",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pa_arrays = []\n",
    "errors = []\n",
    "\n",
    "for path in paths:\n",
    "    print(path)\n",
    "    try:\n",
    "#         pa_arrays.append(parse_peppi.get_slp(path))\n",
    "        pa_arrays.append(parse_libmelee.get_slp(path))\n",
    "    except InvalidGameError as e:\n",
    "        errors.append((path, e))\n",
    "len(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36e7efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_size(array, level) -> dict:\n",
    "    nest = array_to_nest(array)\n",
    "    pickled = pickle.dumps(nest)\n",
    "    compressed = zlib.compress(pickled, level=level)\n",
    "    return len(compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9541bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "totals = {}\n",
    "times = {}\n",
    "for level in range(zlib.Z_BEST_COMPRESSION + 1):\n",
    "    start = time.perf_counter()\n",
    "    totals[level] = sum(nested_size(a, level) for a in pa_arrays)\n",
    "    times[level] = time.perf_counter() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d01388",
   "metadata": {},
   "outputs": [],
   "source": [
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7b5c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5449389",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_nested_size = sum(map(nested_size, pa_arrays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c68c707",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes['pickle_zlib'] = total_nested_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bbb4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in sorted(sizes, key=lambda n: sizes[n]):\n",
    "    print(name, '%.3f' % (sizes['slp'] / sizes[name]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
